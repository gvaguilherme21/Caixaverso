# Caixaverso
# üõ°Ô∏è Detec√ß√£o de Fraude em Cart√µes de Cr√©dito

> **Status do Projeto:** ‚úÖ Conclu√≠do (Aprovado com Excel√™ncia)

Este projeto apresenta uma solu√ß√£o completa de Machine Learning para identificar transa√ß√µes financeiras fraudulentas. O foco central foi resolver o problema do **desbalanceamento extremo de classes** maximizando a detec√ß√£o de fraudes (Recall) sem prejudicar a experi√™ncia de clientes leg√≠timos.

---

## üíº 1. O Problema de Neg√≥cio
A fraude em cart√µes de cr√©dito gera preju√≠zos bilion√°rios anuais. Para uma institui√ß√£o financeira, o desafio √© duplo:
1.  **Detectar a Fraude (Evitar Preju√≠zo):** Bloquear compras ileg√≠timas.
2.  **Evitar Fric√ß√£o (Experi√™ncia do Cliente):** N√£o bloquear compras leg√≠timas (Falsos Positivos).

**O Desafio dos Dados:**
O dataset utilizado cont√©m transa√ß√µes onde **menos de 1% s√£o fraudes**. Modelos tradicionais falham nesse cen√°rio, pois tendem a alcan√ßar 99% de acur√°cia apenas dizendo que "tudo √© leg√≠timo", ignorando o crime.

---

## üõ†Ô∏è 2. Estrat√©gia da Solu√ß√£o
A solu√ß√£o foi desenvolvida em Python seguindo um pipeline rigoroso de Ci√™ncia de Dados:

### üîπ Pipeline e Preven√ß√£o de Data Leakage
Utilizamos `IMBLEARN Pipelines` para garantir que o pr√©-processamento (OneHotEncoding, Scaling) e as t√©cnicas de balanceamento fossem aplicadas corretamente dentro da valida√ß√£o cruzada, evitando vazamento de dados do treino para o teste.

### üîπ Modelagem e Compara√ß√£o
Testamos diferentes arquiteturas para entender qual se adaptava melhor √† complexidade dos dados:
* **KNN (K-Nearest Neighbors):** Baseline baseada em dist√¢ncia.
* **Random Forest:** Modelo de *Bagging* robusto.
* **XGBoost (Campe√£o):** Modelo de *Gradient Boosting* otimizado.

### üîπ Estrat√©gia de Balanceamento
Evolu√≠mos de uma abordagem de **Undersampling** (r√°pida para testes) para o uso de **Class Weights (scale_pos_weight)** no modelo final. Isso permitiu treinar o modelo com **todos os dados dispon√≠veis**, penalizando erros na classe minorit√°ria (fraude) sem descartar informa√ß√µes valiosas.

---

## üìä 3. Resultados Obtidos
O modelo final (XGBoost Otimizado) apresentou performance superior, validada via **RandomizedSearchCV**:

| M√©trica | Resultado | Interpreta√ß√£o |
| :--- | :--- | :--- |
| **AUC-ROC** | **~0.98** | Excelente capacidade de separa√ß√£o entre fraude e n√£o-fraude. |
| **Otimiza√ß√£o** | `max_depth=6` | Profundidade controlada para evitar *Overfitting*. |

> **Nota:** Optamos pela m√©trica AUC-ROC ao inv√©s da Acur√°cia, pois em dados desbalanceados a acur√°cia √© uma m√©trica enganosa.

---

## üß† 4. Explicabilidade (SHAP Values)
N√£o basta prever, √© preciso explicar. Utilizamos **SHAP (SHapley Additive exPlanations)** para abrir a "caixa preta" do modelo.
* **Insight:** O modelo identificou que o valor da transa√ß√£o (`amt`) e a categoria de compra s√£o os maiores preditores de risco, alinhando-se com a intui√ß√£o de neg√≥cio.

<img width="780" height="940" alt="image" src="https://github.com/user-attachments/assets/a239f1ac-c2f6-407b-97ca-ac7e6c80e4bb" />


---

## üöÄ 5. Como Executar o Projeto

### Pr√©-requisitos
* Python 3.8+
* Jupyter Notebook

### Passo a Passo
1. Clone o reposit√≥rio:
   ```bash
   git clone [https://github.com/SEU_USUARIO/NOME_DO_REPO.git](https://github.com/SEU_USUARIO/NOME_DO_REPO.git)
